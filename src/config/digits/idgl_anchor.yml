# Data
data_type: 'uci'
dataset_name: 'digits'
pretrained: null
task_type: 'classification'

# Output
out_dir: '../out/digits/idgl_anchor'



seed: 7545


n_train: 50
n_val: 100

# Model architecture
model_name: 'GraphClf'

# Scalable graph learning
scalable_run: True
num_anchors: 1500 # 1500!



hidden_size: 16


# Bert configure
use_bert: False



# Regularization
dropout: 0.5 # 0.5!
feat_adj_dropout: 0 # 0
gl_dropout: 0.4 # 0.4!


# Graph neural networks
bignn: False
graph_module: 'gcn'
graph_type: 'dynamic' # 'static', 'dynamic'
graph_learn: True
graph_metric_type: 'weighted_cosine' # weighted_cosine, kernel, attention, gat_attention, cosine
graph_include_self: False
graph_skip_conn: 0.3 # 0.25, IL: 0.3!
update_adj_ratio: 0.3 # 0.3!
graph_learn_regularization: True
smoothness_ratio: 0.4 # kernel: 0.5, weighted_cosine: 0.4!
degree_ratio: 0.1 # kernel: 0.1, weighted_cosine: 0.1!
sparsity_ratio: 0 # kernel: 0.3, weighted_cosine: 0!
graph_learn_ratio: 0 # 0
input_graph_knn_size: 24 # cosine-KNN-GCN: 25, kernel: 25, weighted_cosine: 24!
graph_learn_hidden_size: null # kernel: 20, attention: 20
graph_learn_epsilon: 0.65 # kernel: 0.8, weighted_cosine: 0.65!
graph_learn_topk: null # kernel: 15, attn-GCN: 40
# graph_learn_hidden_size2: null # kernel: 20
# graph_learn_epsilon2: 0.65 # kernel: 0.8, weighted_cosine: 0.65
# graph_learn_topk2: null
graph_learn_num_pers: 8 # weighted_cosine: 8!, attention: 1, gat_attention: 8
graph_hops: 2

# GAT only
gat_nhead: 8
gat_alpha: 0.2


# Training
optimizer: 'adam' # best
learning_rate: 0.01 # adam: 0.01, radam: 0.05
weight_decay: 0.0005 # adam: 0.0005, radam: 0.0001
lr_patience: 2
lr_reduce_factor: 0.5 # GCN: 0.5
grad_clipping: null
grad_accumulated_steps: 1
eary_stop_metric: 'nloss' # negative loss
pretrain_epoch: 120 # IL: 120!
max_iter: 10
eps_adj: 1.e-4 # 1.e-4!




# note: RL is not used
rl_ratio: 0  # use mixed objective if > 0; ratio of RL in the loss function
rl_ratio_power: 1  # increase rl_ratio by **= rl_ratio_power after each epoch; (0, 1]
rl_start_epoch: 1  # start RL at which epoch (later start can ensure a strong baseline)?
max_rl_ratio: 0.99
rl_reward_metric: 'acc'
rl_wmd_ratio: 0


shuffle: True # Whether to shuffle the examples during training
max_epochs: 10000
patience: 100
verbose: 20
print_every_epochs: 500 # Print every X epochs


# Testing
out_predictions: False # Whether to output predictions
save_params: True # Whether to save params
logging: True # Turn it off for Codalab


# Device
no_cuda: False
cuda_id: 0
